{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from Case import Case\n",
    "\n",
    "def objective_maker_linear_regression(train_model):\n",
    "    def objective(trial):\n",
    "        model = LinearRegression()\n",
    "        return train_model(model)\n",
    "    return objective\n",
    "\n",
    "for missing_value in [\"drop\",\"fill\"]:\n",
    "    for feature_selection in [\"all\",\"correlation\"]:\n",
    "        case = Case(LinearRegression,data_missing_value=missing_value,feature_selection=feature_selection,normalization=True)\n",
    "        # 1 trial as their are no hyperparameters to tune\n",
    "        case.find_params_with_optuna(objective_maker_linear_regression,number_of_trials=1)\n",
    "        case.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from Case import Case\n",
    "\n",
    "def objective_maker_logistic_regression(train_model):\n",
    "    def objective(trial):\n",
    "        LR_params = {\n",
    "            \"max_iter\": trial.suggest_int(\"max_iter\", 10, 100_000, step=10),\n",
    "            \"n_jobs\": -1,\n",
    "        }\n",
    "        model = LogisticRegression(**LR_params)\n",
    "        return train_model(model)\n",
    "    return objective\n",
    "\n",
    "for missing_value in [\"drop\",\"fill\"]:\n",
    "    for feature_selection in [\"all\",\"correlation\"]:\n",
    "        case = Case(LogisticRegression,data_missing_value=missing_value,feature_selection=feature_selection,normalization=True)\n",
    "        case.find_params_with_optuna(objective_maker_logistic_regression,number_of_trials=1000)\n",
    "        case.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from Case import Case\n",
    "\n",
    "def objective_maker_xgb(train_model):\n",
    "    def objective(trial):\n",
    "        xgb_params = {\n",
    "            \"verbosity\": 0,\n",
    "            \"objective\": \"binary:logistic\",\n",
    "            # use exact for small dataset.\n",
    "            \"tree_method\": \"exact\",\n",
    "            # defines booster, gblinear for linear functions.\n",
    "            # \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n",
    "            \"booster\": \"gbtree\",\n",
    "            # L2 regularization weight.\n",
    "            \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "            # L1 regularization weight.\n",
    "            \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "            # sampling ratio for training data.\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n",
    "            # sampling according to each tree.\n",
    "            # \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 5, 100, step=1),\n",
    "        }\n",
    "\n",
    "        xgb_params['missing'] = 1 #Avoid error\n",
    "\n",
    "        if xgb_params[\"booster\"] in [\"gbtree\", \"dart\"]:\n",
    "            # maximum depth of the tree, signifies complexity of the tree.\n",
    "            xgb_params[\"max_depth\"] = trial.suggest_int(\"max_depth\", 1, 20, step=1)\n",
    "            # minimum child weight, larger the term more conservative the tree.\n",
    "            xgb_params[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 1, 10)\n",
    "            xgb_params[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n",
    "            # defines how selective algorithm is.\n",
    "            xgb_params[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
    "            xgb_params[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "\n",
    "        if xgb_params[\"booster\"] == \"dart\":\n",
    "            xgb_params[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
    "            xgb_params[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
    "            xgb_params[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n",
    "            xgb_params[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n",
    "        \n",
    "        model = xgb.XGBClassifier(**xgb_params)\n",
    "        return train_model(model)\n",
    "    return objective\n",
    "\n",
    "for missing_value in [\"drop\",\"fill\"]:\n",
    "    for feature_selection in [\"all\",\"correlation\"]:\n",
    "        case = Case(xgb.XGBClassifier,data_missing_value=missing_value,feature_selection=feature_selection)\n",
    "        case.find_params_with_optuna(objective_maker_xgb,number_of_trials=1000)\n",
    "        case.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from Case import Case\n",
    "\n",
    "def objective_maker_mlp_1l(train_model):\n",
    "    def objective(trial):\n",
    "        mlp_1l_params = {\n",
    "            \"hidden_layer_sizes\": (trial.suggest_int(\"hidden_layer_sizes_l1\", 1,50,step=1)),\n",
    "            \"activation\":  trial.suggest_categorical(\"activation\", [\"identity\", \"logistic\",\"tanh\", \"relu\"]),\n",
    "            \"solver\": trial.suggest_categorical(\"solver\", [\"lbfgs\", \"sgd\",\"adam\"]),\n",
    "            \"alpha\": trial.suggest_float(\"alpha\",0.0000001, 0.1, log=True),\n",
    "            \"batch_size\": trial.suggest_int(\"batch_size\", 2,512,step=2),\n",
    "            \"learning_rate\": trial.suggest_categorical(\"learning_rate\", [\"constant\", \"invscaling\", \"adaptive\"]),\n",
    "            \"learning_rate_init\": trial.suggest_float(\"learning_rate_init\", 0.00000001,0.1),\n",
    "            \"power_t\": trial.suggest_float(\"power_t\", 0.00000001,0.1),\n",
    "            \"max_iter\": trial.suggest_int(\"max_iter\", 100,100000,step=100),\n",
    "            \"early_stopping\":True,\n",
    "            \"n_iter_no_change\":trial.suggest_int(\"n_iter_no_change\", 10,1000,step=10),  }\n",
    "        model = MLPClassifier(**mlp_1l_params)\n",
    "        return train_model(model)\n",
    "    return objective\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "for missing_value in [\"drop\",\"fill\"]:\n",
    "    for feature_selection in [\"all\",\"correlation\"]:\n",
    "        case = Case(MLPClassifier,data_missing_value=missing_value,feature_selection=feature_selection,normalization=True)\n",
    "        case.find_params_with_optuna(objective_maker_mlp_1l,number_of_trials=1000)\n",
    "        case.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from Case import Case\n",
    "\n",
    "def objective_maker_mlp_2l(train_model):\n",
    "    def objective(trial):\n",
    "        mlp_2l_params = {\n",
    "            \"hidden_layer_sizes\": (trial.suggest_int(\"hidden_layer_sizes_l1\", 1,50,step=1),trial.suggest_int(\"hidden_layer_sizes_l2\", 1,50,step=1)),\n",
    "            \"activation\":  trial.suggest_categorical(\"activation\", [\"identity\", \"logistic\",\"tanh\", \"relu\"]),\n",
    "            \"solver\": trial.suggest_categorical(\"solver\", [\"lbfgs\", \"sgd\",\"adam\"]),\n",
    "            \"alpha\": trial.suggest_float(\"alpha\",0.0000001, 0.1, log=True),\n",
    "            \"batch_size\": trial.suggest_int(\"batch_size\", 2,512,step=2),\n",
    "            \"learning_rate\": trial.suggest_categorical(\"learning_rate\", [\"constant\", \"invscaling\", \"adaptive\"]),\n",
    "            \"learning_rate_init\": trial.suggest_float(\"learning_rate_init\", 0.00000001,0.1),\n",
    "            \"power_t\": trial.suggest_float(\"power_t\", 0.00000001,0.1),\n",
    "            \"max_iter\": trial.suggest_int(\"max_iter\", 100,100000,step=100),\n",
    "            \"early_stopping\":True,\n",
    "            \"n_iter_no_change\":trial.suggest_int(\"n_iter_no_change\", 10,1000,step=10),  }\n",
    "        model = MLPClassifier(**mlp_2l_params)\n",
    "        return train_model(model)\n",
    "    return objective\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "for missing_value in [\"drop\",\"fill\"]:\n",
    "    for feature_selection in [\"all\",\"correlation\"]:\n",
    "        case = Case(MLPClassifier,data_missing_value=missing_value,feature_selection=feature_selection,normalization=True)\n",
    "        case.find_params_with_optuna(objective_maker_mlp_2l,number_of_trials=1000)\n",
    "        case.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from Case import Case\n",
    "\n",
    "def objective_maker_decision_tree(train_model):\n",
    "    def objective(trial):\n",
    "        decision_tree_params = {\n",
    "            \"criterion\":  trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\"]),\n",
    "            \"splitter\": trial.suggest_categorical(\"splitter\", [\"best\", \"random\"]),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 1,10,step=1),\n",
    "            \"min_samples_split\": trial.suggest_float(\"min_samples_split\", 0, 0.8,),\n",
    "            \"min_samples_leaf\": trial.suggest_float(\"min_samples_leaf\", 0, 0.5,),\n",
    "            \"min_weight_fraction_leaf\": trial.suggest_float(\"min_weight_fraction_leaf\", 0, 0.5,),\n",
    "            \"max_features\": trial.suggest_categorical(\"max_features\", [\"auto\", \"sqrt\", \"log2\"]),\n",
    "            \"max_leaf_nodes\": trial.suggest_int(\"max_leaf_nodes\", 2,10,step=1),\n",
    "            \"min_impurity_decrease\": trial.suggest_float(\"min_impurity_decrease\", 0, 0.5,),\n",
    "        }\n",
    "        model = DecisionTreeClassifier(**decision_tree_params)\n",
    "        return train_model(model)\n",
    "    return objective\n",
    "\n",
    "for missing_value in [\"drop\",\"fill\"]:\n",
    "    for feature_selection in [\"all\",\"correlation\"]:\n",
    "        case = Case(DecisionTreeClassifier,data_missing_value=missing_value,feature_selection=feature_selection,normalization=False)\n",
    "        case.find_params_with_optuna(objective_maker_decision_tree,number_of_trials=1000)\n",
    "        case.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from Case import Case\n",
    "\n",
    "def objective_maker_random_forest(train_model):\n",
    "    def objective(trial):\n",
    "        random_forest_params = {\n",
    "            \"n_estimators\":  trial.suggest_int(\"n_estimators\", 10,1000,step=10),\n",
    "            \"criterion\":  trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\"]), #log_loss\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 1,10,step=1),\n",
    "            \"min_samples_split\": trial.suggest_float(\"min_samples_split\", 0, 0.8,),\n",
    "            \"min_samples_leaf\": trial.suggest_float(\"min_samples_leaf\", 0, 0.5,),\n",
    "            \"min_weight_fraction_leaf\": trial.suggest_float(\"min_weight_fraction_leaf\", 0, 0.5,),\n",
    "            \"max_features\": trial.suggest_categorical(\"max_features\", [None, \"sqrt\", \"log2\"]),\n",
    "            \"max_leaf_nodes\": trial.suggest_int(\"max_leaf_nodes\", 2,10,step=1),\n",
    "            \"min_impurity_decrease\": trial.suggest_float(\"min_impurity_decrease\", 0, 0.5,),\n",
    "            \"n_jobs\": -1,\n",
    "        }\n",
    "        model = RandomForestClassifier(**random_forest_params)\n",
    "        return train_model(model)\n",
    "    return objective\n",
    "\n",
    "for missing_value in [\"drop\",\"fill\"]:\n",
    "    for feature_selection in [\"all\",\"correlation\"]:\n",
    "        case = Case(RandomForestClassifier,data_missing_value=missing_value,feature_selection=feature_selection,normalization=False)\n",
    "        case.find_params_with_optuna(objective_maker_random_forest,number_of_trials=1000)\n",
    "        case.report()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
